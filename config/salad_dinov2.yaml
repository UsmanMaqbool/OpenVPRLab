# ----------------------------------------------------------------------------
# Copyright (c) 2024 Amar Ali-bey
# # https://github.com/amaralibey/OpenVPRLab
# Licensed under the MIT License. See LICENSE file in the project root.
# ----------------------------------------------------------------------------


#---------------------------------------------------
# Datamodule Configuration
#---------------------------------------------------
datamodule:
  train_set_name: "gsv-cities-light" # use "gsv-cities" if you have downloaded the full dataset
  train_image_size: 
    - 280
    - 280
  img_per_place: 4
  batch_size: 60
  num_workers: 8
  val_set_names:
    - "msls-val"
    - "pitts30k-val"

#---------------------------------------------------
# VPR Model Configuration
#---------------------------------------------------
backbone:
  # Example of DinoV2
  module: src.models.backbones
  class: DinoV2
  params:
    backbone_name: "dinov2_vitb14" # name of the vit backbone (see DinoV2.AVAILABLE_MODELS)
    num_unfrozen_blocks: 2
    return_cls_token: True

aggregator:
  module: src.models.aggregators # module path
  class: SALAD    # class name in the __init__.py file in the aggregators directory
  params:
    num_channels: 768
    num_clusters: 64
    cluster_dim: 128
    token_dim: 256
    dropout: 0.

#---------------------------------------------------
# Loss Function Configuration
#---------------------------------------------------
loss_function: 
  # check src/losses/vpr_losses.py for available loss functions, we are using pytorch_metric_learning library
  # if you want to develop your own loss function, you can add it to the vpr_losses.py file
  # or create a new file in the losses directory and import it into the __inin__.py file
  module: src.losses
  class: VPRLossFunction
  params:
    loss_fn_name: "MultiSimilarityLoss"   # other possible values: "SupConLoss", "ContrastiveLoss", "TripletMarginLoss"
    miner_name: "MultiSimilarityMiner"    # other possible values: "TripletMarginMiner", "PairMarginMiner"


#---------------------------------------------------
# Trainer Configuration
#---------------------------------------------------
trainer:
  optimizer: adamw
  lr: 0.0001      # learning rate
  wd: 0.001       # weight decay
  warmup: 1500    # linear warmup steps
  max_epochs: 40
  milestones:
    - 10
    - 20
    - 30
  lr_mult: 0.1 # learning rate multiplier at each milestone